- [cloudman docker](#cloudman-docker)
  - [runtime](#runtime)
  - [docker安装 ubuntu](#docker安装-ubuntu)
  - [docker daemon](#docker-daemon)
  - [base镜像](#base镜像)
  - [dockerfile](#dockerfile)
  - [tag](#tag)
  - [容器](#容器)
    - [容器对内存的限制](#容器对内存的限制)
    - [容器对CPU的限制](#容器对cpu的限制)
    - [容器对磁盘io的限制](#容器对磁盘io的限制)
  - [cgroup](#cgroup)
  - [namespace](#namespace)
  - [网络](#网络)
    - [网络类型](#网络类型)
    - [外部连通性](#外部连通性)
  - [存储](#存储)
    - [容器间共享](#容器间共享)
  - [docker 跨主机网络](#docker-跨主机网络)
      - [libnetwork \& CNM](#libnetwork--cnm)
    - [overlay](#overlay)
    - [macvlan](#macvlan)
      - [配置网关router](#配置网关router)
    - [flannel](#flannel)
      - [vxlan](#vxlan)
      - [host-gw](#host-gw)
    - [weave](#weave)
    - [calico](#calico)
    - [方案比较](#方案比较)
      - [网络模型](#网络模型)
      - [Distributed Store](#distributed-store)
      - [IPAM](#ipam)
      - [连通与隔离](#连通与隔离)
      - [性能](#性能)
  - [监控](#监控)
    - [sysdig](#sysdig)
    - [weave scope](#weave-scope)
    - [cAdvisor](#cadvisor)
  - [日志](#日志)
    - [ELK](#elk)
    - [fluentd](#fluentd)
- [cloudman k8s](#cloudman-k8s)
  - [helm](#helm)
    - [chart](#chart)
  - [network policy](#network-policy)
    - [canal](#canal)
  - [dashboard](#dashboard)
  - [prometheus operator](#prometheus-operator)


# cloudman docker

[教程链接](https://mp.weixin.qq.com/s/7o8QxGydMTUe4Q7Tz46Diw)

## runtime

如果大家用过 Java，可以这样来理解 runtime 与容器的关系：

Java 程序就好比是容器，JVM 则好比是 runtime。JVM 为 Java 程序提供运行环境。同样的道理，容器只有在 runtime 中才能运行。

lxc、runc 和 rkt 是目前主流的三种容器 runtime。

lxc 是 Linux 上老牌的容器 runtime。Docker 最初也是用 lxc 作为 runtime。

runc 是 Docker 自己开发的容器 runtime，符合 oci 规范，也是现在 Docker 的默认 runtime。

rkt 是 CoreOS 开发的容器 runtime，符合 oci 规范，因而能够运行 Docker 的容器。

## docker安装 ubuntu

配置 Docker 的 apt 源
1. 安装包，允许 apt 命令 HTTPS 访问 Docker 源。

sudo apt-get install \

    apt-transport-https \

    ca-certificates \

    curl \

    software-properties-common

2. 添加 Docker 官方的 GPG

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

3. 将 Docker 的源添加到 /etc/apt/sources.list

```
sudo add-apt-repository \

  "deb [arch=amd64] https://download.docker.com/linux/ubuntu \

  $(lsb_release -cs) \

  stable"
```

4. 安装 Docker
sudo apt-get update
sudo apt-get install docker-ce

## docker daemon

Docker daemon 运行在 Docker host 上，负责创建、运行、监控容器，构建、存储镜像。

默认配置下，Docker daemon 只能响应来自本地 Host 的客户端请求。如果要允许远程客户端请求，需要在配置文件中打开 TCP 监听，步骤如下：

编辑配置文件 /etc/systemd/system/multi-user.target.wants/docker.service，在环境变量 ExecStart 后面添加 -H tcp://0.0.0.0，允许来自任意 IP 的客户端连接。
重启服务即可

服务器 IP 为 192.168.56.102，客户端在命令行里加上 -H 参数，即可与远程服务器通信。

## base镜像

内核空间是 kernel，Linux 刚启动时会加载 bootfs 文件系统，之后 bootfs 会被卸载掉。

用户空间的文件系统是 rootfs，包含我们熟悉的 /dev, /proc, /bin 等目录。

对于 base 镜像来说，底层直接用 Host 的 kernel，自己只需要提供 rootfs 就行了。

而对于一个精简的 OS，rootfs 可以很小，只需要包括最基本的命令、工具和程序库就可以了。

当容器启动时，一个新的可写层被加载到镜像的顶部。
这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。

所有对容器的改动 - 无论添加、删除、还是修改文件都只会发生在容器层中。

只有容器层是可写的，容器层下面的所有镜像层都是只读的。

Dockerfile 中每一个指令都会创建一个镜像层，上层是依赖于下层的。无论什么时候，只要某一层发生变化，其上面所有层的缓存都会失效。

也就是说，如果我们改变 Dockerfile 指令的执行顺序，或者修改或添加指令，都会使缓存失效。

## dockerfile

Dockerfile 在执行第三步 RUN 指令时失败。我们可以利用第二步创建的镜像 22d31cc52b3e 进行调试，方式是通过 docker run -it 启动镜像的一个容器。

ENTRYPOINT 看上去与 CMD 很像，它们都可以指定要执行的命令及其参数。不同的地方在于 ENTRYPOINT 不会被忽略，一定会被执行，即使运行 docker run 时指定了其他命令。

## tag

每个 repository 可以有多个 tag，而多个 tag 可能对应的是同一个镜像。比如v1 v1.9 v1.9.1可能为同一个镜像打上不同tag

## 容器

docker logs -f 可以持续查看

docker rm -v $(docker ps -aq -f status=exited) # 删除所有处于退出状态的容器

docker pause/unpause 容器

### 容器对内存的限制

docker run -m 200M --memory-swap=300M ubuntu

其含义是允许该容器最多使用 200M 的内存和 100M 的 swap。默认情况下，上面两组参数为 -1，即对容器内存和 swap 的使用没有限制。

下面我们将使用 progrium/stress 镜像来学习如何为容器分配内存。该镜像可用于对容器执行压力测试。执行如下命令：

docker run -it -m 200M --memory-swap=300M progrium/stress --vm 1 --vm-bytes 280M

--vm 1：启动 1 个内存工作线程。

--vm-bytes 280M：每个线程分配 280M 内存。

### 容器对CPU的限制

Docker 可以通过 -c 或 --cpu-shares 设置容器使用 CPU 的权重。如果不指定，默认值为 1024。

与内存限额不同，通过 -c 设置的 cpu share 并不是 CPU 资源的绝对数量，而是一个相对的权重值。某个容器最终能分配到的 CPU 资源取决于它的 cpu share 占所有容器 cpu share 总和的比例。

换句话说：通过 cpu share 可以设置容器使用 CPU 的优先级。

### 容器对磁盘io的限制

注：目前 Block IO 限额只对 direct IO（不使用文件缓存）有效。

默认情况下，所有容器能平等地读写磁盘，可以通过设置 --blkio-weight 参数来改变容器 block IO 的优先级。

--blkio-weight 与 --cpu-shares 类似，设置的是相对权重值，默认为 500。

bps 是 byte per second，每秒读写的数据量。
iops 是 io per second，每秒 IO 的次数。

可通过以下参数控制容器的 bps 和 iops：
--device-read-bps，限制读某个设备的 bps。
--device-write-bps，限制写某个设备的 bps。
--device-read-iops，限制读某个设备的 iops。
--device-write-iops，限制写某个设备的 iops。

下面这个例子限制容器写 /dev/sda 的速率为 30 MB/s

docker run -it --device-write-bps /dev/sda:30MB ubuntu

## cgroup

前面我们看到的--cpu-shares、-m、--device-write-bps 实际上就是在配置 cgroup。

cgroup 到底长什么样子呢？我们可以在 /sys/fs/cgroup 中找到它。还是用例子来说明，启动一个容器，设置 --cpu-shares=512

在 /sys/fs/cgroup/cpu/docker 目录中，Linux 会为每个容器创建一个 cgroup 目录，以容器长ID 命名：

目录中包含所有与 cpu 相关的 cgroup 配置，文件 cpu.shares 保存的就是 --cpu-shares 的配置，值为 512。

同样的，/sys/fs/cgroup/memory/docker 和 /sys/fs/cgroup/blkio/docker 中保存的是内存以及 Block IO 的 cgroup 配置。

## namespace

Mount namespace 让容器看上去拥有整个文件系统。

UTS namespace 让容器有自己的 hostname。 默认情况下，容器的 hostname 是它的短ID，可以通过 -h 或 --hostname 参数设置。

IPC namespace 让容器拥有自己的共享内存和信号量（semaphore）来实现进程间通信，而不会与 host 和其他容器的 IPC 混在一起。

容器拥有自己独立的一套 PID，这就是 PID namespace

Network namespace 让容器拥有自己独立的网卡、IP、路由等资源。

User namespace 让容器能够管理自己的用户，host 不能看到容器中创建的用户。

## 网络

### 网络类型

Docker 提供三种 user-defined 网络驱动：bridge, overlay 和 macvlan。overlay 和 macvlan 用于创建跨主机的网络


到目前为止，容器的 IP 都是 docker 自动从 subnet 中分配，我们能否指定一个静态 IP 呢？

答案是：可以，通过--ip指定。

注：只有使用 --subnet 创建的网络才能指定静态 IP。

**docker network connect 可以将容器添加一张网卡连接到一个虚拟网桥上**


通过 IP 访问容器虽然满足了通信的需求，但还是不够灵活。因为我们在部署应用之前可能无法确定 IP，部署之后再指定要访问的 IP 会比较麻烦。对于这个问题，可以通过 docker 自带的 DNS 服务解决。

从 Docker 1.10 版本开始，docker daemon 实现了一个内嵌的 DNS server，使容器可以直接通过“容器名”通信。方法很简单，只要在启动时用 --name 为容器命名就可以了。

使用 docker DNS 有个限制：只能在 user-defined 网络中使用。也就是说，默认的 bridge 网络是无法使用 DNS 的。


joined 容器是另一种实现容器间通信的方式。

joined 容器非常特别，它可以使两个或多个容器共享一个网络栈，共享网卡和配置信息，joined 容器之间可以通过 127.0.0.1 直接通信。请看下面的例子：

先创建一个 httpd 容器，名字为 web1。

docker run -d -it --name=web1 httpd

然后创建 busybox 容器并通过 --network=container:web1 指定 jointed 容器为 web1

### 外部连通性

容器访问外网，会对出来的包做snat，这与具体端口没有关系，而外网访问容器，则需要指定端口映射，否则无法区分是访问主机还是容器

指定的端口映射，由docker_proxy进行监听，不清楚是否涉及到iptables，但确实会创建dnat规则

## 存储

Docker 支持多种 storage driver，有 AUFS、Device Mapper、Btrfs、OverlayFS、VFS 和 ZFS。它们都能实现分层的架构，同时又有各自的特性。

因为 volume 实际上是 docker host 文件系统的一部分，所以 volume 的容量取决于文件系统当前未使用的空间，目前还没有方法设置 volume 的容量。

bind mount 是将 host 上已存在的目录或文件 mount 到容器。**主要是挂载进容器，容器中生成的新文件也可以在外部主机看到，无法直接挂载容器中的文件到宿主机，只能在宿主机上准备文件或目录对容器进行覆盖**

使用 bind mount 单个文件的场景是：只需要向容器添加文件，不希望覆盖整个目录。在上面的例子中，我们将 html 文件加到 apache 中，同时也保留了容器原有的数据。

使用单一文件有一点要注意：host 中的源文件必须要存在，不然会当作一个新目录 bind mount 给容器。

docker cp 可以在容器和 host 之间拷贝数据，当然我们也可以直接通过 Linux 的 cp 命令复制到 /var/lib/docker/volumes/xxx。

数据卷 的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会复制到数据卷中（仅数据卷为空时会复制） **主要是将容器中文件进行持久化**

批量删除不用的volume
docker volume rm $(docker volume ls -q) 

### 容器间共享

第一种方法是将共享数据放在 bind mount 中，然后将其 mount 到多个容器。

另一种在容器之间共享数据的方式是使用 volume container

其他容器可以通过 --volumes-from 使用 vc_data 这个 volume container

另一种在容器之间共享数据的方式是 data-packed volume container

在上一节的例子中 volume container 的数据归根到底还是在 host 里，有没有办法将数据完全放到 volume container 中，同时又能与其他容器共享呢？

当然可以，通常我们称这种容器为 data-packed volume container。其原理是将数据打包到镜像中，然后通过 docker managed volume 共享。

## docker 跨主机网络

跨主机网络方案包括：

docker 原生的 overlay 和 macvlan。

第三方方案：常用的包括 flannel、weave 和 calico。

如此众多的方案是如何与 docker 集成在一起的？

答案是：libnetwork 以及 CNM。

#### libnetwork & CNM

libnetwork 是 docker 容器网络库，最核心的内容是其定义的 Container Network Model (CNM)，这个模型对容器网络进行了抽象，由以下三类组件组成：

Sandbox

Sandbox 是容器的网络栈，包含容器的 interface、路由表和 DNS 设置。 Linux Network Namespace 是 Sandbox 的标准实现。Sandbox 可以包含来自不同 Network 的 Endpoint。

Endpoint

Endpoint 的作用是将 Sandbox 接入 Network。Endpoint 的典型实现是 veth pair，后面我们会举例。一个 Endpoint 只能属于一个网络，也只能属于一个 Sandbox。

Network

Network 包含一组 Endpoint，同一 Network 的 Endpoint 可以直接通信。Network 的实现可以是 Linux Bridge、VLAN 等。

下面是 CNM 的示例：
![](http://mmbiz.qpic.cn/mmbiz_png/Hia4HVYXRicqHpH8y0uwPo7xIHs4UP99pxRUShjX1RE8FkMfj8rDvslmzx0jYlUqttwkgjJODl91NZfMFYr9nMHA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

libnetwork CNM 定义了 docker 容器的网络模型，按照该模型开发出的 driver 就能与 docker daemon 协同工作，实现容器网络。docker 原生的 driver 包括 none、bridge、overlay 和 macvlan，第三方 driver 包括 flannel、weave、calico 等。

### overlay

注册consul

修改docker daemon 的配置文件/etc/systemd/system/docker.service

docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap

--cluster-store 指定 consul 的地址。
--cluster-advertise 告知 consul 自己的连接地址。

在一个docker主机上创建overlay网络，经由consul同步可以在另一台docker主机上看到

IPAM 是指 IP Address Management

创建overlay网络，添加容器到网络之后，docker会自动创建bridge网络docker_gwbridge，使容器能够连接到外网，而docker容器会拥有两个网卡，一个连接到docker_gwbridge，一个用于overlay通信

可见 overlay 网络中的容器可以直接通信，同时 docker 也实现了 DNS 服务。

下面我们讨论一下 overlay 网络的具体实现：

**docker 会为每个 overlay 网络创建一个独立的 network namespace，其中会有一个 linux bridge br0，endpoint 还是由 veth pair 实现，一端连接到容器中（即 eth0），另一端连接到 namespace 的 br0 上。**

**br0 除了连接所有的 endpoint，还会连接一个 vxlan 设备，用于与其他 host 建立 vxlan tunnel。** 容器之间的数据就是通过这个 tunnel 通信的。
逻辑网络拓扑结构如图所示：
![](/reference/pic/docker_overlay.jpg)

要查看 overlay 网络的 namespace 可以在 host1 和 host2 上执行 ip netns（请确保在此之前执行过 ln -s /var/run/docker/netns /var/run/netns）

### macvlan

macvlan 本身是 linux kernel 模块，其功能是允许在同一个物理网卡上配置多个 MAC 地址，即多个 interface，每个 interface 可以配置自己的 IP。macvlan 本质上是一种网卡虚拟化技术，Docker 用 macvlan 实现容器网络就不奇怪了。

macvlan 的最大优点是性能极好，相比其他实现，macvlan 不需要创建 Linux bridge，而是直接通过以太 interface 连接到物理网络。下面我们就来创建一个 macvlan 网络。

我们会使用 host1 和 host2 上单独的网卡 enp0s9 创建 macvlan。为保证多个 MAC 地址的网络包都可以从 enp0s9 通过，我们需要打开网卡的混杂模式。

ip link set enp0s9 promisc on

因为 host1 和 host2 是 VirtualBox 虚拟机，还需要在网卡配置选项页中设置混杂模式。

![](http://mmbiz.qpic.cn/mmbiz_png/Hia4HVYXRicqGwvkvzuKmxzkxYNicerWtaeibqVD0mq6oHZictRD5ytgmE8wp5Sw9KpRzd3Q4Z7WdDcVxd4vVwU1lgQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

注意：在 host2 中也要执行相同的命令。

① -d macvlan 指定 driver 为 macvlan。

② macvlan 网络是 local 网络，为了保证跨主机能够通信，用户需要自己管理 IP subnet。

③ 与其他网络不同，docker 不会为 macvlan 创建网关，这里的网关应该是真实存在的，否则容器无法路由。

④ -o parent 指定使用的网络 interface。

可见 docker 没有为 macvlan 提供 DNS 服务，这点与 overlay 网络是不同的。

**容器的 eth0 就是 enp0s9 通过 macvlan 虚拟出来的 interface。容器的 interface 直接与主机的网卡连接，这种方案使得容器无需通过 NAT 和端口映射就能与外网直接通信（只要有网关），在网络上与其他独立主机没有区别。**

macvlan 会独占主机的网卡，也就是说一个网卡只能创建一个 macvlan 网络，否则会报错

但主机的网卡数量是有限的，如何支持更多的 macvlan 网络呢？

好在 macvlan 不仅可以连接到 interface（如 enp0s9），也可以连接到 **sub-interface**（如 enp0s9.xxx）。

VLAN 是现代网络常用的网络虚拟化技术，它可以将物理的二层网络划分成多达 4094 个逻辑网络，这些逻辑网络在二层上是隔离的，每个逻辑网络（即 VLAN）由 VLAN ID 区分，VLAN ID 的取值为 1-4094。

Linux 的网卡也能支持 VLAN（apt-get install vlan），同一个 interface 可以收发多个 VLAN 的数据包，不过前提是要创建 VLAN 的 sub-interface。

比如希望 enp0s9 同时支持 VLAN10 和 VLAN20，则需创建 sub-interface enp0s9.10 和 enp0s9.20。

在交换机上，如果某个 port 只能收发单个 VLAN 的数据，该 port 为 Access 模式，如果支持多 VLAN，则为 Trunk 模式，所以接下来实验的前提是：

enp0s9 要接在交换机的 trunk 口上。不过我们用的是 VirtualBox 虚拟机，则不需要额外配置了。

不同 macvlan 网络之间不能通信。但更准确的说法应该是：不同 macvlan 网络不能 在二层上 通信。在三层上可以通过网关将 macvlan 连通，下面我们就**启用网关**。

我们会将 Host 192.168.56.101 配置成一个虚拟路由器，设置网关并转发 VLAN10 和 VLAN20 的流量。当然也可以使用物理路由器达到同样的效果。首先确保操作系统 IP Forwarding 已经启用。

![](/reference/pic/docker_macvlan_router.jpg)

#### 配置网关router

我们会将 Host 192.168.56.101 配置成一个虚拟路由器，设置网关并转发 VLAN10 和 VLAN20 的流量。当然也可以使用物理路由器达到同样的效果。首先确保操作系统 IP Forwarding 已经启用。

输出为 1 则表示启用，如果为 0 可通过如下命令启用：

```

sysctl -w net.ipv4.ip_forward=1

在 /etc/network/interfaces 中配置 vlan sub-interface：

auto eth2

iface eth2 inet manual

auto eth2.10

iface eth2.10 inet manual

vlan-raw-device eth2

auto eth2.20

iface eth2.20 inet manual

vlan-raw-device eth2


启用 sub-interface：

ifup eth2.10

ifup eth2.20


将网关 IP 配置到 sub-interface：

ifconfig eth2.10 172.16.10.1 netmask 255.255.255.0 up

ifconfig eth2.20 172.16.20.1 netmask 255.255.255.0 up


添加 iptables 规则，转发不同 VLAN 的数据包。

iptables -t nat -A POSTROUTING -o eth2.10 -j MASQUERADE

iptables -t nat -A POSTROUTING -o eth2.20 -j MASQUERADE

iptables -A FORWARD -i eth2.10 -o eth2.20 -m state --state RELATED,ESTABLISHED -j ACCEPT

iptables -A FORWARD -i eth2.20 -o eth2.10 -m state --state RELATED,ESTABLISHED -j ACCEPT

iptables -A FORWARD -i eth2.10 -o eth2.20 -j ACCEPT

iptables -A FORWARD -i eth2.20 -o eth2.10 -j ACCEPT

```

### flannel

有关flannel更多介绍见k8s

#### vxlan

注册etcd，各主机安装flannel，最后将docker连接到flannel上
![网络拓扑图](http://mmbiz.qpic.cn/mmbiz_png/Hia4HVYXRicqEnUdGRABYyDB8XGZSjvhp4bEuHEE9poW1hbuOfPSBje9Wbe0Od6kZN544ayjaRwiada89zI2amJBA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

docker连接flannel步骤：
编辑 host1 的 Docker 配置文件 /etc/systemd/system/docker.service，设置 --bip 和 --mtu。
这两个参数的值必须与 /run/flannel/subnet.env 中 FLANNEL_SUBNET 和FLANNEL_MTU 一致。
systemctl daemon-reload
systemctl restart docker.service

flannel 是没有 **DNS** 服务的，容器无法通过 hostname 通信。

flannel 为每个主机分配了独立的 subnet，但 flannel.1 将这些 subnet 连接起来了，相互之间可以路由。本质上，flannel 将各主机上相互独立的 docker0 容器网络组成了一个互通的大网络，实现了容器跨主机通信。flannel 没有提供隔离。

因为 flannel 网络利用的是默认的 bridge 网络，所以容器与外网的连通方式与 bridge 网络一样，即：
容器通过 docker0 NAT 访问外网
通过主机端口映射，外网可以访问容器

#### host-gw

下面对 host-gw 和 vxlan 这两种 backend 做个简单比较。

host-gw 把每个主机都配置成**网关**，主机知道其他主机的 subnet 和转发地址。vxlan 则在主机间建立隧道，不同主机的容器都在一个大的网段内（比如 10.2.0.0/16）。

虽然 vxlan 与 host-gw 使用不同的机制建立主机之间连接，但对于容器则无需任何改变，bbox1 仍然可以与 bbox2 通信。

由于 vxlan 需要对数据进行额外打包和拆包，性能会稍逊于 host-gw。


### weave

weave 是 Weaveworks 开发的容器网络解决方案。weave 创建的虚拟网络可以将部署在多个主机上的容器连接起来。对容器来说，weave 就像一个巨大的**以太网交换机**，所有容器都被接入这个交换机，容器可以直接通信，无需 NAT 和端口映射。除此之外，weave 的 **DNS** 模块使容器可以通过 hostname 访问。

weave 不依赖`分布式数据库`（例如 etcd 和 consul）交换网络信息，每个主机上只需运行 weave 组件就能建立起跨主机容器网络。我们会在 host1 和 host2 上部署 weave 并实践 weave 的各项特性。

weave 安装非常简单，在 host1 和 host2 上执行如下命令：
curl -L git.io/weave -o /usr/local/bin/weave
chmod a+x /usr/local/bin/weave

在 host1 中执行 weave launch 命令，启动 weave 相关服务。weave 的所有组件都是以容器方式运行的，weave 会从 docker hub 下载最新的 image 并启动容器。

weave 运行了三个容器：
weave 是主程序，负责建立 weave 网络，收发数据 ，提供 DNS 服务等。
weaveplugin 是 libnetwork CNM driver，实现 Docker 网络。
weaveproxy 提供 Docker 命令的代理服务，当用户运行 Docker CLI 创建容器时，它会自动将容器添加到 weave 网络。

在 host1 中运行容器 bbox1：
eval $(weave env)
docker run --name bbox1 -itd busybox

首先执行 `eval $(weave env)` 很重要，其作用是将后续的 docker 命令发给 weave proxy 处理。如果要恢复之前的环境，可执行 eval $(weave env --restore)。

bbox1 有两个网络接口 eth0 和 ethwe，其中 eth0 连接的是默认 bridge 网络，即网桥 docker0。
现在我们重点分析 ethwe。从命名和分配的 IP 10.32.0.1/12 可以猜测 ethwe 与 weave 相关

这里出现了多个新 interface：
① vethwe-bridge 与 vethwe-datapath 是 veth pair。
② vethwe-datapath 的父设备（master）是 datapath。
③ datapath 是一个 openvswitch。
④ vxlan-6784 是 vxlan interface，其 master 也是 datapath，weave 主机间是通过 VxLAN 通信的。

![网络结构](/reference/pic/weave.jpg)

weave 网络包含两个虚拟交换机：Linux bridge weave 和 Open vSwitch datapath，veth pair vethwe-bridge 和 vethwe-datapath 将二者连接在一起。
weave 和 datapath 分工不同，weave 负责将容器接入 weave 网络，datapath 负责在主机间 `VxLAN` 隧道中并收发数据。

![网络结构2](/reference/pic/weave2.jpg)

bbox1、bbox2 和 bbox3 的 IP 分别为 10.32.0.1/12、10.32.0.2/12 和 10.44.0.0/12，注意掩码为 12 位，实际上这三个 IP 位于同一个 subnet 10.32.0.0/12。通过 host1 和 host2 之间的 VxLAN 隧道，三个容器逻辑上是在同一个 LAN 中的，当然能直接通信了。

**默认配置下，weave 使用一个大 subnet（例如 10.32.0.0/12），所有主机的容器都从这个地址空间中分配 IP，因为同属一个 subnet，容器可以直接通信。** 如果要实现网络隔离，可以通过环境变量 WEAVE_CIDR 为容器分配不同 subnet 的 IP
![](http://mmbiz.qpic.cn/mmbiz_png/Hia4HVYXRicqFiaQzvAXe0WfIo1CCsbZlgCZOmhQnmGR2dJ2xiaicoWLUsdNuX1ZsVpWsOe0g5sJNbINEVP5Kv4cVQw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
![](http://mmbiz.qpic.cn/mmbiz_png/Hia4HVYXRicqFiaQzvAXe0WfIo1CCsbZlgCdK4gFeQqSvUkiaSRjicyEItIzbWrxJSYeYy2CYfCBiaNPsrunjACmgrKg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)


weave 是一个私有的 VxLAN 网络，默认与外部网络隔离。外部网络如何才能访问到 weave 中的容器呢？
答案是：
首先将主机加入到 weave 网络。
然后把`主机`当作访问 weave 网络的网关。
要将主机加入到 weave，执行 `weave expose`。

这个 IP 10.32.0.3 会被配置到 host1 的 weave 网桥上。
![](http://mmbiz.qpic.cn/mmbiz_png/Hia4HVYXRicqHhd95CxfZGFxNDdTodMNFQicicvkJGsVRUBy3wnpBwTtmulstEvZSctL81PawCaYVCFtRHibdQDaBEA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

weave 网桥位于 `root namespace`，它负责将容器接入 weave 网络。给 weave 配置同一 subnet 的 IP 其本质就是将 host1 接入 weave 网络。 host1 现在已经可以直接与同一 weave 网络中的容器通信了，无论容器是否位于 host1。

接下来要让其他非 weave 主机访问到 bbox1 和 bbox3，只需将网关指向 host1。例如在 192.168.56.101 上添加如下路由：
ip route add 10.32.0.0/12 via 192.168.56.104
其他宿主机配置`额外路由`，与`端口映射`的方案不同

通过上面的配置我们实现了外网到 weave 这个方向的通信，反方向呢？
其实答案很简单：因为容器本身就挂在`默认的 bridge 网络`上，docker0 已经实现了 NAT，所以容器无需额外配置就能访问外网。

10.32.0.0/12 是 weave 网络使用的默认 subnet，如果此地址空间与现有 IP 冲突，可以通过 `--ipalloc-range` 分配特定的 subnet。
weave launch --ipalloc-range 10.2.0.0/16
不过请确保所有 host 都使用相同的 subnet。

### calico

Calico 是一个`纯三层`的虚拟网络方案，Calico 为每个容器分配一个 IP，每个 host 都是 router，把不同 host 的容器连接起来。与 VxLAN 不同的是，Calico 不对数据包做额外封装，不需要 NAT 和端口映射，扩展性和性能都很好。

与其他容器网络方案相比，Calico 还有一大优势：`network policy`。用户可以动态定义 ACL 规则，控制进出容器的数据包，实现业务需求。

Calico 依赖 `etcd` 在不同主机间共享和交换信息，存储 Calico 网络状态。我们将在 host 192.168.56.101 上运行 etcd。

Calico 网络中的每个主机都需要运行 Calico 组件，提供容器 interface 管理、动态路由、动态 ACL、报告状态等功能。

步骤：
修改 host1 和 host2 的 Docker daemon 配置文件 /etc/systemd/system/docker.service， 连接 etcd：
--cluster-store=etcd://192.168.56.101:2379

重启 Docker daemon。
systemctl daemon-reload
systemctl restart docker.service

在 host1 或 host2 上执行如下命令创建 calico 网络 cal_ent1：
docker network create --driver calico --ipam-driver calico-ipam cal_net1

--driver calico 指定使用 calico 的 libnetwork CNM driver。
--ipam-driver calico-ipam 指定使用 calico 的 IPAM driver 管理 IP。

calico 为 global 网络，etcd 会将 cal_net 同步到所有主机。

calico的外网访问：
分配给容器一个虚拟IP，连接到calico在主机的接口，其他宿主机设置通过添加到虚拟IP的路由，实现访问容器，与flannel的host-gw、weave的方案相似，需要二层连通

![calico网络结构](/reference/pic/calico.jpg)

**calico 默认的 policy 规则是：容器只能与同一个 calico 网络中的容器通信。**

calico 的每个网络都有一个同名的 profile，profile 中定义了该网络的 policy。我们具体看一下 cal_net1 的 profile：
calicoctl get profile cal_net1 -o yaml

![profile](/reference/pic/calico2.jpg)

① 命名为 cal_net1，这就是 calico 网络 cal_net1 的 profile。
② 为 profile 添加一个 tag cal_net1。注意，这个 tag 虽然也叫 cal_net1，其实可以随便设置，这跟上面的 name: cal_net1 没有任何关系。此 tag 后面会用到。
③ egress 对从容器发出的数据包进行控制，当前没有任何限制。
④ ingress 对进入容器的数据包进行限制，当前设置是接收来自 tag cal_net1 的容器，根据第 ① 步设置我们知道，实际上就是只接收本网络的数据包，这也进一步解释了前面的实验结果。

既然这是`默认 policy`，那就有方法`定制 policy`，这也是 calico 较其他网络方案最大的特性。

我们没有特别配置，calico 会为自动为网络分配 subnet，当然我们也可以定制。
首先定义一个 IP Pool，比如：
```
cat << EOF | calicoctl create -f -
- apiVersion: v1
 kind: ipPool
 metadata:
   cidr: 17.2.0.0/16
EOF
```

用此 IP Pool 创建 calico 网络。
docker network create --driver calico --ipam-driver calico-ipam --subnet=17.2.0.0/16 my_net

### 方案比较

我们将从如下几个方面比较，大家可以根据不同场景选择最合适的方案。

网络模型
采用何种网络模型支持 multi-host 网络？

Distributed Store 
是否需要 etcd 或 consul 这类分布式 key-value 数据库存储网络信息？

IPMA
如何管理容器网络的 IP？

连通与隔离
提供怎样的网络连通性？支持容器间哪个级别和哪个类型的隔离？

性能
性能比较。

#### 网络模型

跨主机网络意味着将不同主机上的容器用同一个虚拟网络连接起来。这个虚拟网络的拓扑结构和实现技术就是网络模型。

Docker overlay 如名称所示，是 overlay 网络，建立主机间 VxLAN 隧道，原始数据包在发送端被封装成 VxLAN 数据包，到达目的后在接收端解包。

Macvlan 网络在二层上通过 VLAN 连接容器，在三层上依赖外部网关连接不同 macvlan。数据包直接发送，不需要封装，属于 underlay 网络。

Flannel 我们讨论了两种 backend：vxlan 和 host-gw。vxlan 与 Docker overlay 类似，属于 overlay 网络。host-gw 将主机作为网关，依赖三层 IP 转发，不需要像 vxlan 那样对包进行封装，属于 underlay 网络。

Weave 是 VxLAN 实现，属于 overlay 网络。

#### Distributed Store

Docker Overlay、Flannel 和 Calico 都需要 etcd 或 consul。Macvlan 是简单的 local 网络，不需要保存和共享网络信息。Weave 自己负责在主机间交换网络配置信息，也不需要 Distributed Store。

#### IPAM

Docker Overlay 网络中所有主机共享同一个 subnet，容器启动时会顺序分配 IP，可以通过 --subnet 定制此 IP 空间。

Macvlan 需要用户自己管理 subnet，为容器分配 IP，不同 subnet 通信依赖外部网关。

Flannel 为每个主机自动分配独立的 subnet，用户只需要指定一个大的 IP 池。不同 subnet 之间的路由信息也由 Flannel 自动生成和配置。

Weave 的默认配置下所有容器使用 10.32.0.0/12 subnet，如果此地址空间与现有 IP 冲突，可以通过 --ipalloc-range 分配特定的 subnet。

Calico 从 IP Pool（可定制）中为每个主机分配自己的 subnet。

#### 连通与隔离

同一 Docker Overlay 网络中的容器可以通信，但不同网络之间无法通信，要实现跨网络访问，只有将容器加入多个网络。与外网通信可以通过 docker_gwbridge 网络。

Macvlan 网络的连通或隔离完全取决于二层 VLAN 和三层路由。

不同 Flannel 网络中的容器直接就可以通信，没有提供隔离。与外网通信可以通过 bridge 网络。

Weave 网络默认配置下所有容器在一个大的 subnet 中，可以自由通信，如果要实现隔离，需要为容器指定不同的 subnet 或 IP。与外网通信的方案是将主机加入到 weave 网络，并把主机当作网关。

Calico 默认配置下只允许位于同一网络中的容器之间通信，但通过其强大的 Policy 能够实现几乎任意场景的访问控制。

#### 性能

性能测试是一个非常严谨和复杂的工程，这里我们只尝试从技术方案的原理上比较各方案的性能。

最朴素的判断是：**Underlay 网络性能优于 Overlay 网络**。

Overlay 网络利用隧道技术，将数据包封装到 UDP 中进行传输。因为涉及数据包的封装和解封，存在额外的 CPU 和网络开销。虽然几乎所有 Overlay 网络方案底层都采用 Linux kernel 的 vxlan 模块，这样可以尽量减少开销，但这个开销与 Underlay 网络相比还是存在的。所以 Macvlan、Flannel host-gw、Calico 的性能会优于 Docker overlay、Flannel vxlan 和 Weave。

Overlay 较 Underlay 可以支持更多的二层网段，能更好地利用已有网络，以及有避免物理交换机 MAC 表耗尽等优势，所以在方案选型的时候需要综合考虑。

## 监控

### sysdig

sysdig 是一个轻量级的系统监控工具，同时它还原生支持容器。通过 sysdig 我们可以近距离观察 linux 操作系统和容器的行为。
Linux 上有很多常用的监控工具，比如 strace，tcpdump，htop， iftop，lsof ......
而 sysdig 则是将这些工具的功能集成到一个工具中，并且提供一个友好统一的操作界面。

下面我们将演示 sysdig 强大的监控能力。
安装和运行 sysdig 的最简单方法是运行 Docker 容器，命令行为：
```
docker container run -it --rm --name=sysdig --privileged=true \
          --volume=/var/run/docker.sock:/host/var/run/docker.sock \
          --volume=/dev:/host/dev \
          --volume=/proc:/host/proc:ro \
          --volume=/boot:/host/boot:ro \
          --volume=/lib/modules:/host/lib/modules:ro \
          --volume=/usr:/host/usr:ro \
          sysdig/sysdig
```
sysdig 的特点如下：
监控信息全，包括 Linux 操作系统和容器。
界面交互性强。
不过 sysdig 显示的是实时数据，看不到变化和趋势。而且是命令行操作方式，需要 ssh 到 Host 上执行

启动后，通过 `docker container exec -it sysdig bash` 进入容器，执行 `csysdig` 命令，将以交互方式启动 sysdig。

### weave scope

Weave Scope 的最大特点是会自动生成一张 Docker 容器地图，让我们能够直观地理解、监控和控制容器。

### cAdvisor

cAdvisor 是 google 开发的容器监控工具，我们来看看 cAdvisor 有什么能耐。
在 host 中运行 cAdvisor 容器。
```
docker run \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:rw \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  --detach=true \
  --name=cadvisor \
  google/cadvisor:latest
```
通过 `http://[Host_IP]:8080` 访问 cAdvisor。

由于 cAdvisor 提供的操作界面略显简陋，而且需要在不同页面之间跳转，并且只能监控一个 host，这不免会让人质疑它的实用性。但 cAdvisor 的一个亮点是它可以将监控到的数据导出给第三方工具，由这些工具进一步加工处理。

我们可以把 cAdvisor 定位为一个`监控数据收集器`，收集和导出数据是它的强项，而非展示数据。

## 日志

容器启动时可以通过 --log-driver 指定使用的 logging driver。如果要设置 Docker 默认的 logging driver，需要修改 Docker daemon 的启动脚本，指定 --log-driver 参数，比如：
ExecStart=/usr/bin/dockerd -H fd:// --log-driver=syslog --log-opt ......

### ELK

`docker run -p 5601:5601 -p 9200:9200 -p 5044:5044 -it --name elk sebp/elk`

我们使用的是 sebp/elk 这个现成的 image，里面已经包含了整个 ELK stack。容器启动后 ELK 各组件将分别监听如下端口：

5601 - Kibana web 接口
9200 - Elasticsearch JSON 接口
5044 - Logstash 日志接收接口

先访问一下 `Kibana http://[Host IP]:5601/` 看看效果。

几乎所有的软件和应用都有自己的日志文件，容器也不例外。前面我们已经知道 Docker 会将容器日志记录到 `/var/lib/docker/containers/<contariner ID>/<contariner ID>-json.log`，那么只要我们能够将此文件发送给 ELK 就可以实现日志管理。

要实现这一步其实不难，因为 ELK 提供了一个配套小工具 Filebeat，它能将指定路径下的日志文件转发给 ELK。同时 Filebeat 很聪明，它会监控日志文件，当日志更新时，Filebeat 会将新的内容发送给 ELK。

### fluentd

`docker run -d -p 24224:24224 -p 24224:24224/udp -v /data:/fluentd/log fluent/fluentd`

这里我们用 Filebeat 将 Fluentd 收集到的日志转发给 Elasticsearch。这当然不是唯一的方案，Fluentd 有一个 plugin fluent-plugin-elasticsearch 可以直接将日志发送给 Elasticsearch。

启动测试容器。
```
docker run -d \
           --log-driver=fluentd \
           --log-opt fluentd-address=localhost:24224 \
           --log-opt tag="log-test-container-A" \
           busybox sh -c 'while true; do echo "This is a log message from container A"; sleep 10; done;'

docker run -d \
           --log-driver=fluentd \
           --log-opt fluentd-address=localhost:24224 \
           --log-opt tag="log-test-container-B" \
           busybox sh -c 'while true; do echo "This is a log message from container B"; sleep 10; done;'

--log-driver=fluentd 告诉 Docker 使用 Fluentd 的 logging driver。
--log-opt fluentd-address=localhost:24224 将容器日志发送到 Fluentd 的数据接收端口。
--log-opt tag="log-test-container-A" 和 --log-opt tag="log-test-container-B" 在日志中添加一个可选的 tag，用于区分不同的容器。
```

# cloudman k8s

## helm

Helm 有两个重要的概念：`chart` 和 `release`。

chart 是创建一个应用的信息集合，包括各种 Kubernetes 对象的配置模板、参数定义、依赖关系、文档说明等。chart 是应用部署的自包含逻辑单元。可以将 chart 想象成 apt、yum 中的软件`安装包`。

release 是 chart 的`运行实例`，代表了一个正在运行的应用。当 chart 被安装到 Kubernetes 集群，就生成一个 release。chart 能够多次安装到同一个集群，每次安装都是一个 release。

Helm 是包管理工具，这里的包就是指的 chart。Helm 能够：
从零创建新 chart。
与存储 chart 的仓库交互，拉取、保存和更新 chart。
在 Kubernetes 集群中安装和卸载 release。
更新、回滚和测试 release。
Helm 包含两个组件：`Helm 客户端` 和 `Tiller 服务器`。

![](https://mmbiz.qpic.cn/mmbiz_png/Hia4HVYXRicqEy4saORQ4PrBj5fW40oNwtQWeFnf7wTYCC3WTxClkQ6QI1ugEuygic5ZSZn7JNj31OCQsSBwKUgmg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

Helm 客户端是终端用户使用的命令行工具，用户可以：
在本地开发 chart。
管理 chart 仓库。
与 Tiller 服务器交互。
在远程 Kubernetes 集群上安装 chart。
查看 release 信息。
升级或卸载已有的 release。

Tiller 服务器运行在 Kubernetes 集群中，它会处理 Helm 客户端的请求，与 Kubernetes API Server 交互。Tiller 服务器负责：
监听来自 Helm 客户端的请求。
通过 chart 构建 release。
在 Kubernetes 中安装 chart，并跟踪 release 的状态。
通过 API Server 升级或卸载已有的 release。

简单的讲：**Helm 客户端负责管理 chart；Tiller 服务器负责管理 release**。

helm 有很多子命令和参数，为了提高使用命令行的效率，通常建议安装 helm 的 `bash 命令补全脚本`，方法如下：

`helm completion bash > .helmrcecho "source .helmrc" >> .bashrc`
重新登录后就可以通过 Tab 键补全 helm 子命令和参数了。

### chart

chart 是 Helm 的`应用打包格式`。chart 由一系列文件组成，这些文件描述了 Kubernetes 部署应用时所需要的资源，比如 Service、Deployment、PersistentVolumeClaim、Secret、ConfigMap 等。

单个的 chart 可以非常简单，只用于部署一个服务，比如 Memcached；chart 也可以很复杂，部署整个应用，比如包含 HTTP Servers、 Database、消息中间件、cache 等。

chart 将这些文件放置在预定义的目录结构中，通常整个 chart 被打成 `tar` 包，而且标注上版本信息，便于 Helm 部署。

一旦安装了某个 chart，我们就可以在 `~/.helm/cache/archive` 中找到 chart 的 tar 包

`templates` 目录 
各类 Kubernetes 资源的配置模板都放置在这里。Helm 会将 `values.yaml` 中的参数值注入到模板中生成标准的 YAML 配置文件。

Helm 采用了 Go 语言的模板来编写 chart。Go 模板非常强大，支持变量、对象、函数、流控制等功能。

如果存在一些信息多个模板都会用到，则可在 `templates/_helpers.tpl` 中将其定义为子模板，然后通过 templates 函数引用。

根据 chart 的最佳实践，所有资源的名称都应该保持一致，对于我们这个 chart，无论 Secret 还是 Deployment、PersistentVolumeClaim、Service，它们的名字都是子模板 mysql.fullname 的值。

作为准备工作，安装之前需要先清楚 chart 的使用方法。这些信息通常记录在 values.yaml 和 README.md 中。除了下载源文件查看，执行 `helm inspect values` 可能是更方便的方法。

除了接受 values.yaml 的默认值，我们还可以定制化 chart，比如设置 mysqlRootPassword。

Helm 有两种方式传递配置参数：

指定自己的 values 文件。
通常的做法是首先通过 `helm inspect values mysql > myvalues.yaml`生成 values 文件，然后设置 mysqlRootPassword，之后执行 `helm install --values=myvalues.yaml mysql`。

通过 `--set` 直接传入参数值

只要是程序就会有 bug，chart 也不例外。Helm 提供了 `debug` 的工具：`helm lint` 和 `helm install --dry-run --debug`。

helm lint 会检测 chart 的语法，报告错误以及给出建议。

helm install --dry-run --debug 会模拟安装 chart，并输出每个模板生成的 YAML 内容。

执行 `helm create mychart` 的命令创建 chart

## network policy

`Network Policy` 是 Kubernetes 的一种资源。Network Policy 通过 Label 选择 Pod，并指定其他 Pod 或外界如何与这些 Pod 通信。

默认情况下，所有 Pod 是`非隔离`的，即任何来源的网络流量都能够访问 Pod，没有任何限制。当为 Pod 定义了 Network Policy，只有 Policy 允许的流量才能访问 Pod。

不过，不是所有的 Kubernetes 网络方案都支持 Network Policy。比如 Flannel 就不支持，Calico 是支持的。我们接下来将用 `Canal` 来演示 Network Policy。Canal 这个开源项目很有意思，它用 `Flannel` 实现 Kubernetes 集群网络，同时又用 `Calico` 实现 Network Policy。

### canal

部署 Canal 与部署其他 Kubernetes 网络方案非常类似，都是在执行了 kubeadm init 初始化 Kubernetes 集群之后通过 kubectl apply 安装相应的网络方案。也就是说，没有太好的办法直接切换使用不同的网络方案，基本上只能重新创建集群。

## dashboard

Kubernetes 默认没有部署 Dashboard，可通过如下命令安装：

`kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml`
Dashboard 会在 kube-system namespace 中创建自己的 Deployment 和 Service。


## prometheus operator

Prometheus Operator 是 CoreOS 开发的基于 Prometheus 的 Kubernetes 监控方案，也可能是目前功能最全面的开源方案。

安装 Prometheus Operator Deployment
`helm install --name prometheus-operator --set rbacEnable=true --namespace=monitoring helm/prometheus-operator`
Prometheus Operator 所有的组件都打包成 Helm Chart，安装部署非常方便